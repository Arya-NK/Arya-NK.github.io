---
layout: post
title: "The story of Big Data, Data Science & Data Mining"
excerpt: ""
tags: [Big Data, Data Science, Data Mining]
feature: http://www.nmgncp.com/data/out/45/3881185-big-data-wallpaper.jpg
comments: true
---
We live in a world where the amount of data has increased in a large scale in various fields. Like websites track the user activities, marketers collect the customer’s purchasing history, so called “Quantified-Selfers” collect the user’s sleeping patterns, heart rates and so on.

With the availability of vast amount of data, most industries started focusing on leveraging valuable insights from it. Those insights can make businesses more competitive advantage in the market and even early detection of certain diseases in the medical field.
The way insights get unleashed from data is by systematic analysis which involve various phases. But before stepping into that, let’s look back to understand from where this all started.


## Back in 1890s

The emergence of enormous data led to the question “**How to effectively organise and manage big datasets**”.  One of the such data problem was 1980s US census data, probably the first data problem in North America. To manage the US census data more effectively, an American inventor Herman Hollerith used punch card which stores the data for each person and designed a machine which can read back the data from punch card.

The success of Hollerith’s machine, led to the establishment of his own company “The Tabulating Machine Company” which is merged with other two companies and later known as International Business Machines Corporation (IBM).

## 20th century

The [1935 Social Security Act](https://www.ssa.gov/history/briefhistory3.html) which launched by US government under the administration of Franklin D. Roosevelt was one of such enormous data gathering project. The winning contractor of Social Security Act was IBM. IBM gathered the employment records of 23 million employees and stored the information in punch cards.

Later in year 1943, at Bletchey park, a British facility responsible for deciphering Nazi code during the world war 2 developed the first programmable electronic digital machine named the [Colossus](http://www.colossus-computer.com/colossus1.html). Although Colossus was designed for a specific cryptanalysis task, it provided reliable high-speed computations and thereby reduced the amount of time from weeks to hours.

## Information Explosion

Before 1950, most of the challenges associated with data was about volume until the US National Security Agency (NSA) recognised the second key component of Big data which is velocity. During the cold war, cryptologists in NSA struggled with the massive information overload due to automated data collection.

In 1961, Derek Price described the exponential growth rate of scientific journals & papers in his publication '[*Science Since Babylon*](http://derekdesollaprice.org/science-since-babylon/)'. Price’s work highlighted the fact that big data is not just about volume but also about velocity.

In order to manage the increasing volume of data, Harry J. Gray and Henry Ruston came up with recommendations in their paper '*Techniques for Coping with the Information Explosion in 1964*'.

As an alternative to limiting the amount of data, B. A Marron and P.A.D. de Maine introduced Automatic data compression, a system which can automatically compress the data and decode it when it is required.

In 1974, the term "**data science**" has been freely used by Peter Naur, a Danish computer scientist and Turing award winner in his book titled ‘*Concise Survey of Computer Methods in Sweden and the United States*’. Naur defined data science as "**_The science of dealing with data, once they have been established, while the relation of the data to what they represent is delegated to other fields and sciences._**"

Followed by, The International Association for Statistical Computing (IASC) was founded in 1977 to transform data in the form of knowledge and information by consolidating the traditional statistical methodology, computer technology, and domain expertise.

Data science started recognising in the industry gradually. The future significance of data science was recognised during the International Federation of Classification Societies (IFCS) conference taken place in 1996 where the conference titled '*Datascience, classification, and related methods*'. Since the objective of classification society is to support the wide range developments across classification and other related knowledge ordering, the usage of terms data analysis, data science, data mining started appearing on their publications.

1997 was the year in which Big Data and Data mining were emerged. The journal '*Knowledge Discovery and Data Mining*' explored the theory, techniques and the relevant practices for extracting knowledge from large database. In the same year, Michael Cox and David Ellsworth coined the term ’big data’ in their paper '*Application-controlled demand paging for out-of-core visualisation*'.

The introduction of Hadoop made significant impact to big data revolution and changed the dynamics and economics of large scale computing. Originally, Hadoop was developed for Nutch search engine project by Michael J. Cafarella and Doug Cutting. Eventually, Hadoop enabled the distributed processing of big data sets over clusters of servers with high fault tolerance.

---

# Now, what is Big Data, Data Science & Data mining?

## Big Data

The following are some of the definitions of big data:

Doug Laney in 2001 described big data as being a
> “3-dimensional data challenge of increasing data volume, velocity and variety”.

Later in 2010, Apache Hadoop defined big data as follows
>“datasets which could not be captured, managed, and processed by general computers within an acceptable scope.

Likewise, TechAmerica Foundation’s defined big data as  
>“Big data is a term that describes large volumes of high velocity, complex and variable data that require advanced techniques and technologies to enable the capture, storage, distribution, management, and analysis of the information.”

The point here is that big data refers to massive complex dataset which is difficult to store, analyse and visualise for further processes or results. Whereas **Big Data Analytics** or **Knowledge Discovery from Data (KDD)** defines as some operations designed to get insights and knowledge from big dataset.
Enterprise data is an example for big dataset which is comprised of data from various business functions such as production, inventory, sales, financial and so on. The impact of performing analytics on big dataset can lead enterprises to data-driven activities and decision making. Another common example of big data can be found in Internet of Things (IoT) paradigm. Data from various types of sensors results in big clusters of data.

As already stated, the characteristics of big data make it an extreme challenge for discovering useful insights. To get more intuition about the underlying problems in big data, consider the following perspective:

Imagine that a number of blind men are trying to examine a giant elephant, in which the elephant indicates big data in this context. The aim of each blind men is to draw a picture of elephant based on the part they examined. As each person’s view is limited, their independent conclusion ended up to biased decision such as rope, wall, tree or hose as shown in the figure below.

![](/assets/img/postimages/elephant.png)[The blind men and the giant elephant: the localized (limited) view of each blind man leads to a biased conclusion.]

In this case, to draw a best picture of elephant, heterogeneous information from different sources (blind men) need to get aggregated.

## The 3V’s of Big Data

The definition of big data pointed out the basic three characteristics of big data which are **Volume**, **Velocity** and **Variety**.

* **Volume** points to the magnitude of data, around in multiple terabytes and petabytes.
* **Velocity** indicates the rate at which data are generated and how fast these generated data being processed.
* **Variety** specifies the structural heterogeneity of dataset. The data sets can be structured, semi structured and unstructured data. The data in tabular form which is seen in spreadsheets and in relational database are examples of structured data whereas video, audio, text, images are unstructured. An example for semi structured data is Extensible Markup Language (XML).

<span style="color:green">Is there any universal benchmark for these characteristics?  No.</span>

<span style="color:green">And are these the only characteristics of big dataset? Again the answer is No.</span>

A universal benchmark for volume, variety and velocity doesn’t exist as the limits for these characteristics varies upon size, sector as well as location of the firm. Most importantly, the dimensions of big data are not independent to each other. That means, a change in one dimension is more likely to make change in another dimension as well.

In addition to the basic three Vs, three more dimensions has emerged. They are **Veracity**, **Variability** and **Value**.

* **Veracity**: Veracity was coined by IBM and it represents the unreliability inherent in some sources of data. For example, user sentiments in social media which is uncertain in nature.

* **Variability (and complexity)**: Variability and Complexity are other two added dimensions of big data by SAS. Variability denotes to the change in the data flow rates. Most often, the velocity of big data is not consistent and has periodic peaks and troughs. Complexity points to the big data that generated through numerous sources which would be a critical challenge.

* **Value**: The value is considered as the defining attribute of big data which is introduced by Oracle. Often, the big data in its original form is considered as relatively “low value density”. Higher value is obtained by analysing large volumes of such data.

## Data Science

The term science denotes to systematic study for acquiring knowledge. Likewise, the term Data science implies to the study of the generalisable extraction of knowledge from data. The data can be unstructured and heterogeneous to a large extent like image and video files. Analysing such complex datasets involve appropriate integration and interpretation as well as required to use tools from computer science, linguistics and various other disciplines.

One of the familiar use cases of data science is Amazon storing user searches which further correlates to ‘*what that particular user search for*’ with ‘*what other users search for*’ and by doing so, it outputs surprisingly appropriate recommendations. Another similar scenario is Google’s YouTube searches and recommendations. It’s not just Google and Amazon utilise the data effectively, many other companies including Facebook and LinkedIn recognised the potential impact of data science.

As part of the data analysis, the first step is getting the data into a useable state often known as data cleaning. The data scientists remove unnecessary information and handle missing data in this phase. For example, extracting plain text from HTML tags by using a library called Beautiful Soup. The purpose of data cleaning phase is to improve the quality of data prior to the analysis.

In order to extract interesting pattern, it needs useful features from cleaned data and the way to get those features is by a process called **feature engineering**. In feature engineering, the raw data gets aggregated to some sort of canonical form named features and that better represent the underlying problem to the models, resulting in improved model accuracy.

The model is fundamental to data science. The data scientist uses machine learning and statistics to build model that correlates the data with business outcomes. The outcomes of model could be predictions or recommendations which based on the business question asked.

### Skills for Data Science

The skills required for a data scientist is interdisciplinary.

They can tackle all aspects of a problem, from initial data collection and data conditioning to drawing conclusions.
The part of Economist Hal Varian’s quote states the skills of data scientist :
The ability to take data—to be able to understand it, to process it, to extract value from it, to visualize it, to communicate it—that’s going to be a hugely important skill in the next decades.

## Data Mining

# References

* Dhar, V., 2013. Data science and prediction. Communications of the ACM, 56(12), pp.64-73.
* Gandomi, A. and Haider, M., 2015. Beyond the hype: Big data concepts, methods, and analytics. International Journal of Information Management, 35(2), pp.137-144.
*	Han, J., Pei, J. and Kamber, M., 2011. Data mining: concepts and techniques. Elsevier.
*	Zarate Santovena, A., 2013. Big data: evolution, components, challenges and opportunities (Doctoral dissertation, Massachusetts Institute of Technology).
